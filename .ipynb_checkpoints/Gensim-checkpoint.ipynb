{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "000edf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pprint\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e60c39c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"Human machine interface for lab abc computer applications\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "058d24ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "230b55ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a', 'of', 'the', 'for', 'in', 'to', 'and'}\n"
     ]
    }
   ],
   "source": [
    "# Stop Words\n",
    "stoplist = set('for a of the and to in'.split(' '))\n",
    "print(stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c90762e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'machine', 'interface', 'lab', 'abc', 'computer', 'applications'], ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'management', 'system'], ['system', 'human', 'system', 'engineering', 'testing', 'eps'], ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'], ['generation', 'random', 'binary', 'unordered', 'trees'], ['intersection', 'graph', 'paths', 'trees'], ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering'], ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "# clean the stopwords from corpus\n",
    "\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in text_corpus]\n",
    "\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb2d5406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>,\n",
      "            {'abc': 1,\n",
      "             'applications': 1,\n",
      "             'binary': 1,\n",
      "             'computer': 2,\n",
      "             'engineering': 1,\n",
      "             'eps': 2,\n",
      "             'error': 1,\n",
      "             'generation': 1,\n",
      "             'graph': 3,\n",
      "             'human': 2,\n",
      "             'interface': 2,\n",
      "             'intersection': 1,\n",
      "             'iv': 1,\n",
      "             'lab': 1,\n",
      "             'machine': 1,\n",
      "             'management': 1,\n",
      "             'measurement': 1,\n",
      "             'minors': 2,\n",
      "             'opinion': 1,\n",
      "             'ordering': 1,\n",
      "             'paths': 1,\n",
      "             'perceived': 1,\n",
      "             'quasi': 1,\n",
      "             'random': 1,\n",
      "             'relation': 1,\n",
      "             'response': 2,\n",
      "             'survey': 2,\n",
      "             'system': 4,\n",
      "             'testing': 1,\n",
      "             'time': 2,\n",
      "             'trees': 3,\n",
      "             'unordered': 1,\n",
      "             'user': 3,\n",
      "             'well': 1,\n",
      "             'widths': 1})\n"
     ]
    }
   ],
   "source": [
    "# Count word frequencies\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for word in text:\n",
    "        frequency[word] += 1\n",
    "    \n",
    "pprint.pprint(frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "496e54e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "# only keep words occurs more than once\n",
    "processed_corpus = [[word for word in text if frequency[word] > 1] for text in texts]\n",
    "pprint.pprint(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06a879a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)\n"
     ]
    }
   ],
   "source": [
    "#we want to associate each word in the corpus with a unique integer ID\n",
    "\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c59a1bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computer': 0,\n",
      " 'eps': 8,\n",
      " 'graph': 10,\n",
      " 'human': 1,\n",
      " 'interface': 2,\n",
      " 'minors': 11,\n",
      " 'response': 3,\n",
      " 'survey': 4,\n",
      " 'system': 5,\n",
      " 'time': 6,\n",
      " 'trees': 9,\n",
      " 'user': 7}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d2fe509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1)]\n"
     ]
    }
   ],
   "source": [
    "new_doc = \"Human computer interaction\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "12edbb65",
   "metadata": {},
   "source": [
    "The first entry in each tuple corresponds to the ID of the token in the dictionary, the second corresponds to the count of this token.\n",
    "\n",
    "Note that “interaction” did not occur in the original corpus and so it was not included in the vectorization. Also note that this vector only contains entries for words that actually appeared in the document. Because any given document will only contain a few words out of the many words in the dictionary, words that do not appear in the vectorization are represented as implicitly zero as a space saving measure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9f2b21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)],\n",
      " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
      " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
      " [(1, 1), (5, 2), (8, 1)],\n",
      " [(3, 1), (6, 1), (7, 1)],\n",
      " [(9, 1)],\n",
      " [(9, 1), (10, 1)],\n",
      " [(9, 1), (10, 1), (11, 1)],\n",
      " [(4, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "#We can convert our entire original corpus to a list of vectors:\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "pprint.pprint(bow_corpus)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "43cbf230",
   "metadata": {},
   "source": [
    "The tf-idf model transforms vectors from the bag-of-words representation to a vector space where the frequency counts are weighted according to the relative rarity of each word in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73cf0a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 0.5898341626740045), (11, 0.8075244024440723)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "\n",
    "# train the model\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "# transform the \"system minors\" string\n",
    "words = \"system minors\".lower().split()\n",
    "print(tfidf[dictionary.doc2bow(words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "522fed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[bow_corpus], num_features=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab8dda5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.0), (1, 0.32448703), (2, 0.41707572), (3, 0.7184812), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "query_document = 'system engineering'.split()\n",
    "query_bow = dictionary.doc2bow(query_document)\n",
    "sims = index[tfidf[query_bow]]\n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d24465a9",
   "metadata": {},
   "source": [
    "How to read this output? Document 3 has a similarity score of 0.718=72%, document 2 has a similarity score of 42% etc. We can make this slightly more readable by sorting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78a438b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.7184812\n",
      "2 0.41707572\n",
      "1 0.32448703\n",
      "0 0.0\n",
      "4 0.0\n",
      "5 0.0\n",
      "6 0.0\n",
      "7 0.0\n",
      "8 0.0\n"
     ]
    }
   ],
   "source": [
    "for document_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):\n",
    "    print(document_number, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62e011d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55c5bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b74c7fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d194f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43631cde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c787d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc02b41e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5551974d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
